{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download() #select all-copora - Large Collection of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=brown.sents(categories='adventure')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4637"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #no of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 sentence\n",
    "' '.join(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline - Convert Text to Numeric\n",
    "\n",
    "Here it is called bag because the order of the words dont matter\n",
    "\n",
    "- Get the data/corpus\n",
    "\n",
    "- Tokenisation , Stopward Removal:\n",
    "  Tokensization referes to breaking up the documents to sentences then to finally words\n",
    "  Stopward Removal: Removing unuseful words like I,the,a,an,you etc.,.\n",
    "  \n",
    "- Stemming: Convert different words having same meaning to a common word. Like runs, running, ran => run\n",
    "\n",
    "- Building a Vocab\n",
    "\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation , Stopward Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to the market to buy \n",
    "            some fruits.\"\"\"\n",
    "\n",
    "sentence='Send all the 50 documents related to chapters 1,2,3 at manjari.jain@bofa.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy \\n            some fruits.']\n",
      "3\n",
      "It was a very pleasant day.\n"
     ]
    }
   ],
   "source": [
    "sents=sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))\n",
    "print(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'manjari.jain@bofa.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_1=sentence.split()\n",
    "words_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'manjari.jain',\n",
       " '@',\n",
       " 'bofa.com']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_2=word_tokenize(sentence)\n",
    "words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='Send all the 50 documents related to chapters 1,2,3 at manjari.jain@bofa.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'manjari', 'jain', 'bofa', 'com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "useful_words=tokenizer.tokenize(sentence)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'manjari', 'jain@bofa', 'com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z@]+')\n",
    "useful_words=tokenizer.tokenize(sentence)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'manjari.jain', 'bofa.com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z.]+')\n",
    "useful_words=tokenizer.tokenize(sentence)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'manjari.jain@bofa.com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_words=tokenizer.tokenize(sentence)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopward Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'does', 'shouldn', 'd', 'over', 'some', \"aren't\", 'her', 'while', 'so', 'o', 'won', 'my', 'your', 'it', 'on', 'where', 'we', 'for', 'doesn', \"wouldn't\", 'above', 'by', 'hasn', 'them', 'ain', \"mightn't\", 'haven', 'very', 'yours', 'any', 'do', 'but', \"you'd\", 'under', 'now', 'y', 'ourselves', 'itself', 'themselves', 'most', 'theirs', 'of', 'was', 'yourself', 'in', 'to', 'ours', 'when', 'few', 'him', \"you've\", 'did', \"couldn't\", 'from', 'here', 'hadn', \"mustn't\", 'needn', 'i', \"you're\", \"you'll\", 'both', 'no', 'just', 'only', 'don', \"weren't\", \"won't\", 'whom', \"she's\", 'have', 'our', 'weren', 'herself', 'again', 'against', 'once', 'if', 'can', 'they', 'mightn', 'the', 'will', 'than', \"needn't\", 'aren', 'couldn', 'its', \"don't\", 'he', \"shan't\", 'such', 'other', \"that'll\", 'nor', 'am', 'be', 'each', 't', \"doesn't\", 'and', 'mustn', 'these', 'after', 'should', 'myself', 'or', 'until', 'as', \"didn't\", 'you', 'a', 'before', 'out', 'up', 'who', 'his', 'doing', 'through', 'this', 'that', 're', 'with', 'me', 'between', 'not', 'which', 'didn', 'isn', 'himself', 'those', 'has', 'why', 's', \"it's\", 'what', 'further', 'same', \"isn't\", 'during', 'wouldn', 'all', 'having', 've', \"hadn't\", \"hasn't\", 'about', 'below', 'being', 'at', 'too', 'off', 'shan', 'ma', 'hers', 'own', 'yourselves', 'is', 'down', 'had', 'because', \"haven't\", 'were', 'their', 'more', 'an', 'into', 'then', 'how', 'are', \"shouldn't\", 'there', \"wasn't\", 'm', 'll', 'been', \"should've\", 'wasn'}\n"
     ]
    }
   ],
   "source": [
    "sw=set(stopwords.words('english'))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence,sw):\n",
    "    useful_words=[w for w in sentence if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi,', 'My', 'name', 'is', 'Manjari', 'Jain.', 'I', 'am', 'an', 'Iron', 'Man.', 'I', 'am', 'not', 'a', 'superman']\n"
     ]
    }
   ],
   "source": [
    "text=\"Hi, My name is Manjari Jain. I am an Iron Man. I am not a superman\".split()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi,', 'My', 'name', 'Manjari', 'Jain.', 'I', 'Iron', 'Man.', 'I', 'superman']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(text,sw))\n",
    "\n",
    "# here it removes \"not\" word also. Meaning it can effect our negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence,sw):\n",
    "    useful_words=[w for w in sentence if w==\"not\" or w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi,', 'My', 'name', 'Manjari', 'Jain.', 'I', 'Iron', 'Man.', 'I', 'not', 'superman']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(text,sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "- Process that transforms particular words (verbs,plurals) into radical form\n",
    "- Preserves the semantics of the sentences without increasing the no of tokens\n",
    "- Example: jumps, jumping, jumped, jump => jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Fox love to make jumps. The quick brown fox was seen jumping over the lovely dog from a 6ft high wall.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''There are three types of Stemmers provided by the nltk- Snowball, Porter, Lancas Stemmer'''\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Porter Stemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'),ps.stem('jumped'),ps.stem('jumps'),ps.stem('jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('love', 'love')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely'),ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Snowball Stemmer\n",
    "\n",
    "ss=SnowballStemmer('english')\n",
    "ss.stem('jumping'),ss.stem('jumped'),ss.stem('jumps'),ss.stem('jump'),ss.stem('jumpful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('love', 'love')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely'),ss.stem('loving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lementization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumped'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl=WordNetLemmatizer()\n",
    "\n",
    "wl.lemmatize('jumped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing so far learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'India Cricket Team will win world cup says capt. Virat Kohli. World Cup will be held at Sri Lanka',\n",
    "    'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people',\n",
    "    'The movie Razi is an exciting Indian Spy Thriller based up on a real story',\n",
    "    'Fox love to make jumps.', \n",
    "    'The quick brown fox was seen jumping over the lovely dog from a 6ft high wall.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India Cricket Team will win world cup says capt. Virat Kohli. World Cup will be held at Sri Lanka'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenization(text):\n",
    "    tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def final_token(sent_list):\n",
    "    words=[]\n",
    "    for sent in sent_list:\n",
    "        new_words=tokenization(sent)\n",
    "        new_words=[w.lower() for w in new_words]\n",
    "        words.extend(new_words)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'cricket', 'team', 'will', 'win', 'world', 'cup', 'says', 'capt', 'virat', 'kohli', 'world', 'cup', 'will', 'be', 'held', 'at', 'sri', 'lanka', 'we', 'will', 'win', 'next', 'lok', 'sabha', 'elections', 'says', 'confident', 'indian', 'pm', 'the', 'nobel', 'laurate', 'won', 'the', 'hearts', 'of', 'the', 'people', 'the', 'movie', 'razi', 'is', 'an', 'exciting', 'indian', 'spy', 'thriller', 'based', 'up', 'on', 'a', 'real', 'story', 'fox', 'love', 'to', 'make', 'jumps', 'the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lovely', 'dog', 'from', 'a', 'ft', 'high', 'wall'] 75\n"
     ]
    }
   ],
   "source": [
    "words=final_token(corpus)\n",
    "print(words,len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwards Removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw=set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words,sw):\n",
    "    useful_words=[w for w in words if w==\"not\" or w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'cricket', 'team', 'win', 'world', 'cup', 'says', 'capt', 'virat', 'kohli', 'world', 'cup', 'held', 'sri', 'lanka', 'win', 'next', 'lok', 'sabha', 'elections', 'says', 'confident', 'indian', 'pm', 'nobel', 'laurate', 'hearts', 'people', 'movie', 'razi', 'exciting', 'indian', 'spy', 'thriller', 'based', 'real', 'story', 'fox', 'love', 'make', 'jumps', 'quick', 'brown', 'fox', 'seen', 'jumping', 'lovely', 'dog', 'ft', 'high', 'wall'] 51\n"
     ]
    }
   ],
   "source": [
    "new_words=remove_stopwords(words,sw)\n",
    "print(new_words,len(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stemming(words):\n",
    "    res=[]\n",
    "    for w in words:\n",
    "        if ps.stem(w) not in res:\n",
    "            res.append(ps.stem(w))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'cricket', 'team', 'win', 'world', 'cup', 'say', 'capt', 'virat', 'kohli', 'held', 'sri', 'lanka', 'next', 'lok', 'sabha', 'elect', 'confid', 'indian', 'pm', 'nobel', 'laurat', 'heart', 'peopl', 'movi', 'razi', 'excit', 'spi', 'thriller', 'base', 'real', 'stori', 'fox', 'love', 'make', 'jump', 'quick', 'brown', 'seen', 'dog', 'ft', 'high', 'wall'] 43\n"
     ]
    }
   ],
   "source": [
    "stem_words=stemming(new_words)\n",
    "print(stem_words,len(stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India Cricket Team will win world cup says capt. Virat Kohli. World Cup will be held at Sri Lanka'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[\n",
    "    'India Cricket Team will win world cup says capt. Virat Kohli. World Cup will be held at Sri Lanka',\n",
    "    'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people',\n",
    "    'The movie Razi is an exciting Indian Spy Thriller based up on a real story'\n",
    "]\n",
    "\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x43 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 48 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "vectorized_corpus=cv.fit_transform(corpus)\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 2],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus[0] #first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'india': 12, 'cricket': 6, 'team': 33, 'will': 39, 'win': 40, 'world': 42, 'cup': 7, 'says': 29, 'capt': 4, 'virat': 37, 'kohli': 15, 'be': 3, 'held': 11, 'at': 1, 'sri': 31, 'lanka': 16, 'we': 38, 'next': 20, 'lok': 18, 'sabha': 28, 'elections': 8, 'confident': 5, 'indian': 13, 'pm': 25, 'the': 34, 'nobel': 21, 'laurate': 17, 'won': 41, 'hearts': 10, 'of': 22, 'people': 24, 'movie': 19, 'razi': 26, 'is': 14, 'an': 0, 'exciting': 9, 'spy': 30, 'thriller': 35, 'based': 2, 'up': 36, 'on': 23, 'real': 27, 'story': 32}\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "''' It is first finding the length of unique words in all sentences of corpus\n",
    "    It found out there are 43 unique words overall\n",
    "    Then it creates four separate rows: one representing each sentence of length 43\n",
    "    It starts with first sentence\n",
    "(a) Then it counts the no of times 0th Index (\"an\") to 42th Index (\"world\") is coming in the sentence. \n",
    "    Then it moves for second sentence and repeats \"a\" step\n",
    "'''\n",
    "\n",
    "print(cv.vocabulary_)#word=> wrt the indexes\n",
    "print(len(cv.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse Mapping\n",
    "\n",
    "numbers=vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=cv.inverse_transform(numbers) # we are getting unique words from the sentence at 2nd Index\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Stopwords and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stemming(words):\n",
    "    res=[]\n",
    "    for w in words:\n",
    "        if ps.stem(w) not in res:\n",
    "            res.append(ps.stem(w))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words,sw):\n",
    "    useful_words=[w for w in words if w==\"not\" or w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z.@]+')\n",
    "\n",
    "def mytokenizer(text):\n",
    "    words=tokenizer.tokenize(text)\n",
    "    words=remove_stopwords(words,sw)\n",
    "    return stemming(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['send', 'document', 'relat', 'chapter', 'manjari.jain@bofa.com'],\n",
       " 'Send all the 50 documents related to chapters 1,2,3 at manjari.jain@bofa.com')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytokenizer(sentence),sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(tokenizer=mytokenizer)\n",
    "vectorized_corpus=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['capt.', 'cricket', 'cup', 'held', 'india', 'kohli.', 'lanka',\n",
       "        'say', 'sri', 'team', 'virat', 'win', 'world'], dtype='<U8')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse Mapping\n",
    "\n",
    "numbers=vectorized_corpus[0]\n",
    "sent=cv.inverse_transform(numbers) # we are getting unique words from the sentence at 2nd Index\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "test_data=[\n",
    "    'Razi is jumping on the hill'\n",
    "]\n",
    "\n",
    "cv.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'india': 9, 'cricket': 3, 'team': 27, 'win': 30, 'world': 31, 'cup': 4, 'say': 23, 'capt.': 1, 'virat': 29, 'kohli.': 11, 'held': 8, 'sri': 25, 'lanka': 12, 'next': 16, 'lok': 14, 'sabha': 22, 'elect': 5, 'confid': 2, 'indian': 10, 'pm': 19, 'nobel': 17, 'laurat': 13, 'heart': 7, 'peopl': 18, 'movi': 15, 'razi': 20, 'excit': 6, 'spi': 24, 'thriller': 28, 'base': 0, 'real': 21, 'stori': 26}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More ways to create features\n",
    "\n",
    "- Unigrams - Treat every word as the feature\n",
    "- Bigrams - \n",
    "- Trigrams\n",
    "- n-grams\n",
    "- TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1=['This is a good movie']\n",
    "sent_2=['This is not a good movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a good movie', 'This is not a good movie']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=[sent_1[0],sent_2[0]]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc=cv.fit_transform(doc).toarray()\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'not': 3}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bi Gram \n",
    "\n",
    "cv_bigram=CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "cv_bigram.fit_transform(doc)\n",
    "cv_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is good': 3,\n",
       " 'is good movie': 0,\n",
       " 'this is not': 4,\n",
       " 'is not good': 1,\n",
       " 'not good movie': 2}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tri Gram\n",
    "\n",
    "cv_trigram=CountVectorizer(ngram_range=(3,3))\n",
    "cv_trigram.fit_transform(doc)\n",
    "cv_trigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 7,\n",
       " 'this is': 12,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'this is good': 13,\n",
       " 'is good movie': 4,\n",
       " 'not': 8,\n",
       " 'is not': 5,\n",
       " 'not good': 9,\n",
       " 'this is not': 14,\n",
       " 'is not good': 6,\n",
       " 'not good movie': 10}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram\n",
    "\n",
    "cv_ngram=CountVectorizer(ngram_range=(1,3))\n",
    "cv_ngram.fit_transform(doc)\n",
    "cv_ngram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Normalisation (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "- Avoid features that occurs very often as they contain less info\n",
    "- Info decreases as the no of occurence increases across different types of documents\n",
    "- So, we define another term : term-document-frequency which associates a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1=\"this is good movie\"\n",
    "sent_2=\"this was good movie\"\n",
    "sent_3=\"this is not good movie\"\n",
    "\n",
    "corpus=[sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46333427, 0.59662724, 0.46333427, 0.        , 0.46333427,\n",
       "        0.        ],\n",
       "       [0.41285857, 0.        , 0.41285857, 0.        , 0.41285857,\n",
       "        0.69903033],\n",
       "       [0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc=tfidf.fit_transform(corpus).toarray()\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
